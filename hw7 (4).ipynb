{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e54d6def24efa4818b523868d3d9a95f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Linguistics 531<br>\n",
    "Fall 2023<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: Ashwin Raj\n",
    "\n",
    "people I talked to about the assignment: *\\<FILL IN HERE\\>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76be1ecd002ce89fe7ba5f08bac3dbba",
     "grade": false,
     "grade_id": "hw7header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #7*\n",
    "\n",
    "**This is due Tuesday, November 28, 2023 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac1e372e447955b92046a119fe13730",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f42544eac75a5b2661e9a9777dc7569",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "What I need on my machine to properly grade this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6533f7aefbac103db012fa4f2a36aa",
     "grade": false,
     "grade_id": "cell-ea976235a1443dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5f3d329570f012501243607da41c1e",
     "grade": false,
     "grade_id": "cell-d62bb3f494727bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*In the editable cell below, enter the path on your own machine,* then uncomment that line so the notebook works on your machine.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "#newbfile = '/home/ashwinr136/ling531/ling531/NewB/train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1133ceea0b96644bf4754c84814221c1",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Flesh out the function below that reads in the data and extract sentences in class #2 and class #8. (1 point)\n",
    "\n",
    "You'll return a list of sentences in these two classes. Each sentence should be a single string. (You'll write a function to normalize them in question 2.)\n",
    "\n",
    "You should be able to use concepts and code from last week and earlier in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b5b8f0fee94bc53c19d1bada4b50419",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get28(filename):\n",
    "    '''reads in newB data and extracts classes 2 and 8\n",
    "    \n",
    "    args:\n",
    "        filename: location+name of newB file\n",
    "    returns:\n",
    "        s2: a list of sentences of class 2\n",
    "        s8: a list of sentences of class 8\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    sentences = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                publication_id, sentence = int(parts[0]), parts[1].strip()\n",
    "                sentences.append((publication_id, sentence))\n",
    "    s2 = [sentence for code, sentence in sentences if code == 2]\n",
    "    s8 = [sentence for code, sentence in sentences if code == 8]\n",
    "    return s2, s8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaba32249da6b452763db9c8ec7a9e74",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s2,s8 = get28(newbfile)\n",
    "\n",
    "# test 1a, 1pt\n",
    "assert len(s2) == len(s8) == 23071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6704a3b91987387a0d5515ea7590f6d8",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** Write a function to normalize and tokenize your sentences: convert anything besides upper or lower case letters, numbers, and the percent sign to space, and then split the string on white space. (2 points)\n",
    "\n",
    "You should be able to reuse your code from Assignment 6 for this.\n",
    "\n",
    "*This should now be very familiar. You don't need to overthink this. In a real application, we would probably want to do something fancy to handle Roman letters with accents and diacritics, like á and ñ, which **do** occur in the data sets we'll be using, but we're not doing anything fancy here. If it's not an upper case ASCII letter, or a lower case ASCII letter, or a digit, or the percent sign, convert it to space and then split on any white space. A few document IDs to check, to make sure you're properly normalizing, include 47602, 48327, 48677, and 56254. If you're working in Ubuntu, you can open the `train_orig.txt` file in the built-in text editor (gedit) and perform a RegEx-based \"Find and Replace\" search for a pattern like `[^a-zA-Z0-9%\\s]`, in order to find examples of the kinds of characters your function will need to filter out, and the word tokenization that would result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "512911bb9250813995e276453a1d1a92",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    converts anything that is not a letter (upper or lower\n",
    "    case ASCII) or number to space, and tokenizes.\n",
    "    \n",
    "    args:\n",
    "        s: a sentence as a string\n",
    "    returns:\n",
    "        a list of normalized word tokens as strings\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    normalized_text = re.sub(r'[^a-zA-Z0-9%]', ' ', s.lower())\n",
    "    tokens = normalized_text.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d55c198faa715277a93ffe719ba3b87c",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s2sentences = [tokenize(s) for s in s2]\n",
    "s8sentences = [tokenize(s) for s in s8]\n",
    "\n",
    "# test 2a, 1pt\n",
    "assert s8sentences[1214] == ['spurned', 'of', 'a', 'washington', 'dc', 'parade', 'president', 'trump',\n",
    " 'said', 'he', 'now', 'plans', 'to', 'visit', 'paris', 'to', 'see', 'the', 'parade', 'down',\n",
    " 'the', 'avenue', 'des', 'champs', 'lys', 'es', 'to', 'mark', 'the', 'centennial', 'of', 'the',\n",
    " 'end', 'of', 'world', 'war', 'i', 'in', 'mid', 'november']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c33e610d9a54ecdd5b31209bb983dfe5",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1pt\n",
    "assert len(s2sentences[2535]) == 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "278719caf733707a5fc60ff24ddcac20",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Split the normalized and tokenized sentences in these two categories (using `s2sentences` and `s8sentences` that were output from question 2) into training and test sets where the *last* 1000 sentences comprise each test set and the rest make up the training sets. (3 points)\n",
    "\n",
    "Pay attention to the variable names that are in the comments.\n",
    "\n",
    "Again, this should seem very similiar to something you did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1928e3f8a18855addfd7989ac2002082",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#train2 = all but the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "train2 = s2sentences[:-1000]\n",
    "\n",
    "#test2 = the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "test2 = s2sentences[-1000:]\n",
    "\n",
    "#train8 = all but the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "train8 = s8sentences[:-1000]\n",
    "\n",
    "#test8 = the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "test8 = s8sentences[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0211e46708e813c4a732e4ec92783907",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3a, 1pt\n",
    "assert len(train2) == len(train8) == 22071 and len(test2) == len(test8) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bc58a30a90f47cf703576525a48e383",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1pt\n",
    "assert train8[15000][:5] == ['illustration', 'caption', 'sarah',\n",
    "                             'ricegetty', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b34ffd7a56f482130f2a6a3cfba6cad8",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3c, 1pt\n",
    "assert test8[55][:5] == ['a', 'plan', 'being', 'considered', 'by']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "023d904e764817b0ed5e2ff4ceb27a73",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Extract the set of all words in class 2, class 8, and in both classes together. (3 points)\n",
    "\n",
    "You should include both the test and training sets for each class; that is, you can use `s2sentences` and `s8sentences` from question 2 as your input, not the separate test and training sets that were created as output in **Question 3**.\n",
    "\n",
    "*Hint: [Set comprehensions](https://www.pythonforbeginners.com/basics/set-comprehension-in-python) may be helpful here. Also, once you have `vocab2` and `vocab8`, what's an easy way to create `vocab28`?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "018111ca488e58ca7fb40a0726a94424",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#vocab2 = set of words in class 2\n",
    "# YOUR CODE HERE\n",
    "vocab2 = set(word for sentence in s2sentences for word in sentence)\n",
    "\n",
    "#vocab8 = set of all words in class 8\n",
    "# YOUR CODE HERE\n",
    "vocab8 = set(word for sentence in s8sentences for word in sentence)\n",
    "\n",
    "#vocab28 = set of all words in class 2 and 8\n",
    "# YOUR CODE HERE\n",
    "vocab28 = vocab2 | vocab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f12ee00da44ecc349845f255d56f22e4",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This will obviously depend on your normalization function; if you're\n",
    "#  having trouble getting these numbers, check that your normalization\n",
    "#  function is really converting all the appropriate characters to space.\n",
    "\n",
    "# test 4a, 1pt\n",
    "assert len(vocab2) == 20701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "769924a313f7030df3dd48710112e8e7",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4b, 1pt\n",
    "assert len(vocab8) == 21353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaa4182a3d59b2a9c1dc22735be4eae7",
     "grade": true,
     "grade_id": "q4t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4c, 1pt\n",
    "assert len(vocab28) == 27944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c98c0a75c7d3e3c5e5c838762e3dfc97",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** Write a function that will create a multinomial Naive Bayes model from training data. (3 points)\n",
    "\n",
    "The function will take two arguments: a list of normalized and tokenized sentences in some class (that is, `train2` or `train8` from question 3) plus a list of all possible words (in any class&mdash;`vocab28` from question 4). Look at the `assert` statements below to see how the function will behave.\n",
    "\n",
    "The function will return a dictionary from each possible word to its log probability value in that training data. Use add-one smoothing to make sure all possible words have a non-zero (non-infinite in log space) value.\n",
    "\n",
    "*Hint: Are you counting words? A `Counter()` may be a good option for this task. Note that in order to implement smoothing  with a `Counter()`, you may need to tackle this in two steps. Think about the two data structures that are inputs to this function and consider what each of them might help you with for this task. The Review & Mastery activity for this week may give you some hints.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04c03e768c2b2711e4e8dcdaa948f51b",
     "grade": false,
     "grade_id": "q5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeModel(sentences,vocabulary):\n",
    "    '''\n",
    "    returns a multinomial naive bayes model for a list\n",
    "    of sentences and a set of possible words. Values are\n",
    "    expressed as log probabilities.\n",
    "    \n",
    "    Uses add-one smoothing to make sure all possible words\n",
    "    have a non-zero (non-infinite in log space) value.\n",
    "    \n",
    "    args:\n",
    "        sentences: a list of training sentences\n",
    "        vocabulary: a set of (all) possible words\n",
    "    returns:\n",
    "        a dictionary from possible words to that word's log probability\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence)\n",
    "        total_words += len(sentence)\n",
    "        \n",
    "    vocab_size = len(vocabulary)\n",
    "    class_probs = {}\n",
    "    for word in vocabulary:\n",
    "        word_count = word_counts[word]\n",
    "        \n",
    "        smoothed_prob = (word_count + 1) / (total_words + vocab_size)\n",
    "        \n",
    "        log_prob = np.log(smoothed_prob)\n",
    "        class_probs[word] = log_prob\n",
    "\n",
    "    return class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6025d3b27a44522d8400d9a0d5c69bb",
     "grade": true,
     "grade_id": "q5t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model2 = makeModel(train2,vocab28)\n",
    "model8 = makeModel(train8,vocab28)\n",
    "\n",
    "# test 5a, 1pt\n",
    "assert len(model2) == len(model8) == 27944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81b5960f337e35771e218b2e94606057",
     "grade": true,
     "grade_id": "q5t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5b, 1pt\n",
    "# I get -11.313875043456596\n",
    "assert isclose(model2['hats'],-11.3139,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f7e10de3e28858808607d648003903b",
     "grade": true,
     "grade_id": "q5t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5c, 1pt\n",
    "# I get -11.455193945760294\n",
    "assert isclose(model8['hats'], -11.4552, abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f50ffd3c3840faaffcddd9129c7f3bdb",
     "grade": false,
     "grade_id": "q6q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**6.** Fill out the function below to apply a pair of multinomial naive bayes models (that your function in question 5 created) to a list of normalized and tokenized sentences for testing (that is, we'll use `test2` and `test8` for this). (2 points)\n",
    "\n",
    "The function reports the number of sentences that match the second model better.\n",
    "\n",
    "You will *not* need the Bayes correction because there are an equal number of items in both classes.\n",
    "\n",
    "*Hint: If you have a Naive Bayes model, how do you evaluate the predicted probability of a test document according to that model?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09c062cc7a1866fd7d7f48dfb2af78f4",
     "grade": false,
     "grade_id": "q6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def applyModels(modelA,modelB,sentences):\n",
    "    '''\n",
    "    Determine which of two models fits the test data better.\n",
    "    \n",
    "    args:\n",
    "        modelA, modelB: Naive Bayes models as created by makeModel()\n",
    "        sentences: a list of normalized & tokenized sentences\n",
    "            as created by tokenize()\n",
    "    returns:\n",
    "        the number of sentences that fit modelB better\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    better_model_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Calculate the log probability of the sentence under both models\n",
    "        probA = sum(modelA.get(word, 0.0) for word in sentence)\n",
    "        probB = sum(modelB.get(word, 0.0) for word in sentence)\n",
    "\n",
    "        # Compare probabilities and update the counter\n",
    "        if probB > probA:\n",
    "            better_model_count += 1\n",
    "\n",
    "    return better_model_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dead3b59504b73d05a577333bea0e271",
     "grade": true,
     "grade_id": "q6t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note that because we've written this function to retun the number\n",
    "#  of sentences that fit *model B* better, we need to be sure that\n",
    "#  the category for model B always matches the category of the test set.\n",
    "\n",
    "res2 = applyModels(model8,model2,test2)\n",
    "res8 = applyModels(model2,model8,test8)\n",
    "\n",
    "# test 6a, 1pt\n",
    "assert res2 == 729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "980d78bf2083f789aa0735160657cc70",
     "grade": true,
     "grade_id": "q6t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6b, 1pt\n",
    "assert res8 == 645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8782d87c149bc3b870516639038a9c4",
     "grade": false,
     "grade_id": "q7q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**7.** Summarize the results of your multinomial Naive Bayes models and discuss. (2 points)\n",
    "\n",
    "How well did the model do here? How did this model do, compared to the previous two models that we tried (namely, classifying solely by document length, and then classifying based on a normal statistical distribution)? Why do you think you're seeing this kind of performance, with this kind of model?\n",
    "\n",
    "*Hint: If you're not sure how to approach this, re-watch the beginning of video 7.2 and the end of 7.4, paying attention to the assumptions that the multinomial Naive Bayes model makes about the documents within each class. What assumptions did our **previous** models make about the documents within each class? How well do those assumptions hold of this data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beea62aca81f61a18e6f29ddf9ff49c5",
     "grade": true,
     "grade_id": "q7a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The multinomial bayes model based on train2 returns 729 sentences out of 1000 that fit test2 better than model 8, and the model based on train8 returns 645 sentences out of 1000 that fit test8 better than model2. This model had a higher accuracy on the test data compared to classifying solely by document length and by classifying based on a normal distribution. This could be because the Naive Bayes model takes into account the actual words in the document rather than following a probability distribution, which makes the model more complex and in this case results in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
